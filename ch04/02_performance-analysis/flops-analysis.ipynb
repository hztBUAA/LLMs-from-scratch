{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLOPS Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FLOPs (Floating Point Operations Per Second) measure the computational complexity of neural network models by counting the number of floating-point operations executed\n",
    "- High FLOPs indicate more intensive computation and energy consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting thop (from -r requirements-extra.txt (line 1))\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bb/0f/72beeab4ff5221dc47127c80f8834b4bcd0cb36f6ba91c0b1d04a1233403/thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\30616\\anaconda3\\envs\\llms\\lib\\site-packages (from thop->-r requirements-extra.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\30616\\anaconda3\\envs\\llms\\lib\\site-packages (from torch->thop->-r requirements-extra.txt (line 1)) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\30616\\anaconda3\\envs\\llms\\lib\\site-packages (from torch->thop->-r requirements-extra.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\30616\\anaconda3\\envs\\llms\\lib\\site-packages (from torch->thop->-r requirements-extra.txt (line 1)) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\30616\\anaconda3\\envs\\llms\\lib\\site-packages (from torch->thop->-r requirements-extra.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\30616\\anaconda3\\envs\\llms\\lib\\site-packages (from torch->thop->-r requirements-extra.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\30616\\anaconda3\\envs\\llms\\lib\\site-packages (from jinja2->torch->thop->-r requirements-extra.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\30616\\anaconda3\\envs\\llms\\lib\\site-packages (from sympy->torch->thop->-r requirements-extra.txt (line 1)) (1.3.0)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.1.1.post2209072238\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thop version: 0.1.1-2209072238\n",
      "torch version: 2.0.1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import matplotlib\n",
    "import torch\n",
    "\n",
    "print(\"thop version:\", version(\"thop\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GerIdRMXd6g9",
    "outputId": "ccdd5c71-d221-4a84-f9bc-09557e77162d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "# 尝试安装gpu相关的包\n",
    "try:\n",
    "    import torch.cuda\n",
    "    print(\"torch cuda version:\", version(\"torch\"))\n",
    "except ImportError:\n",
    "    print(\"no torch cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between CUDA, GPU, and the NVIDIA driver is foundational to understanding how applications leverage NVIDIA GPUs for computation, especially in contexts like deep learning and scientific computing. Here's a detailed explanation:\n",
    "\n",
    "1. **GPU (Graphics Processing Unit):** This is the hardware component designed to accelerate graphics rendering and computational tasks. NVIDIA's GPUs are widely used for both gaming and compute-intensive applications, such as machine learning, data analysis, and scientific simulations.\n",
    "\n",
    "2. **CUDA (Compute Unified Device Architecture):** CUDA is a parallel computing platform and programming model invented by NVIDIA. It allows developers to use NVIDIA GPUs for general purpose processing (an approach known as GPGPU, General-Purpose computing on Graphics Processing Units). CUDA provides a direct way to interact with the GPU's virtual instruction set and parallel computational elements, for executing compute kernels.\n",
    "\n",
    "3. **NVIDIA Driver:** The NVIDIA driver is software that operates at the system level to enable communication between the operating system and the GPU hardware. It includes the necessary components to interface with CUDA applications, manage GPU resources, and execute the compiled CUDA kernels on the GPU.\n",
    "\n",
    "**Relationship:**\n",
    "\n",
    "- **CUDA and GPU:** CUDA is designed specifically for programming NVIDIA GPUs. It provides APIs and a runtime environment for developers to direct GPU acceleration for their applications. The CUDA platform is supported by CUDA-capable GPUs, which are specifically designed by NVIDIA to support parallel computing tasks using CUDA.\n",
    "\n",
    "- **CUDA and NVIDIA Driver:** The NVIDIA driver includes the CUDA Driver API, which is necessary for executing applications developed with CUDA. The driver must be compatible with the version of CUDA used to develop an application. For example, newer versions of CUDA may require an updated NVIDIA driver that understands the latest CUDA features and instructions.\n",
    "\n",
    "- **GPU and NVIDIA Driver:** The NVIDIA driver is essential for the operating system to recognize and utilize the GPU hardware. It translates high-level commands into low-level instructions for the GPU and manages resource allocation and scheduling for compute tasks.\n",
    "\n",
    "In summary, the NVIDIA GPU is the hardware capable of accelerating computational tasks. CUDA is the software layer that allows developers to write programs that leverage the GPU for parallel computing. The NVIDIA driver acts as the intermediary, enabling the operating system and CUDA applications to communicate with the GPU hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "gpt-small (124M)  : 5.1e+11 FLOPS\n",
      "gpt-medium (355M) : 1.4e+12 FLOPS\n",
      "gpt-large (774M)  : 3.2e+12 FLOPS\n",
      "gpt-xl (1558M)    : 6.4e+12 FLOPS\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile\n",
    "\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "input_tensor = torch.randint(0, 50257, (2, 1024)).to(device)\n",
    "\n",
    "for size in model_configs:\n",
    "    BASE_CONFIG.update(model_configs[size])\n",
    "    \n",
    "    model = GPTModel(BASE_CONFIG).bfloat16()\n",
    "    model.to(device)\n",
    "\n",
    "    # MACS = multiply-accumulate operations\n",
    "    # MACS are typically counted as two FLOPS (one multiply and one accumulate)\n",
    "    macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    flops = 2*macs\n",
    "    print(f\"{size:18}: {flops:.1e} FLOPS\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. PyTorch is using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Print the number of GPUs available\n",
    "    print(f'Number of GPUs available: {torch.cuda.device_count()}')\n",
    "    # Loop through and print each GPU's name\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "else:\n",
    "    print('CUDA is not available. PyTorch is using CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4060 Laptop GPU'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())#cuda是否可用\n",
    "torch.cuda.device_count()#返回GPU的数量\n",
    "torch.cuda.get_device_name(0)#返回gpu名字，设备索引默认从0开始\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
